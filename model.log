AudioLLM(
  (encoder): speechEncoder(
    (global_cmvn): GlobalCMVN()
    (enc): ModuleList(
      (0): Subsampling(
        (core): Conv2dSubsampling4(
          (conv): Sequential(
            (0): Conv2d(1, 1024, kernel_size=(3, 3), stride=(2, 2))
            (1): ReLU()
            (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2))
            (3): ReLU()
          )
          (out): Sequential(
            (0): Linear(in_features=19456, out_features=1024, bias=True)
          )
        )
      )
      (1): Transformer(
        (embed): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.1, inplace=False)
          (3): ReLU()
        )
        (pe): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoders): MultiSequential(
          (0): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (1): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (2): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (3): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (4): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (5): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (6): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (7): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (8): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (9): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (10): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (11): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (12): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (13): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (14): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (15): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (16): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (17): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (18): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (19): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (20): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (21): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (22): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
          (23): TransformerLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=1024, out_features=4096, bias=True)
              (w_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (concat_linear): Identity()
          )
        )
        (after_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (llm_decoder): Qwen2ForCausalLM(
    (model): Qwen2Model(
      (embed_tokens): Embedding(152064, 3584)
      (layers): ModuleList(
        (0-27): 28 x Qwen2DecoderLayer(
          (self_attn): Qwen2SdpaAttention(
            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)
            (k_proj): Linear(in_features=3584, out_features=512, bias=True)
            (v_proj): Linear(in_features=3584, out_features=512, bias=True)
            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)
            (rotary_emb): Qwen2RotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((3584,), eps=1e-06)
      (rotary_emb): Qwen2RotaryEmbedding()
      (h): ModuleList(
        (0-27): 28 x Qwen2DecoderLayer(
          (self_attn): Qwen2SdpaAttention(
            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)
            (k_proj): Linear(in_features=3584, out_features=512, bias=True)
            (v_proj): Linear(in_features=3584, out_features=512, bias=True)
            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)
            (rotary_emb): Qwen2RotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        )
      )
      (wte): Embedding(152064, 3584)
    )
    (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
    (transformer): Qwen2Model(
      (embed_tokens): Embedding(152064, 3584)
      (layers): ModuleList(
        (0-27): 28 x Qwen2DecoderLayer(
          (self_attn): Qwen2SdpaAttention(
            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)
            (k_proj): Linear(in_features=3584, out_features=512, bias=True)
            (v_proj): Linear(in_features=3584, out_features=512, bias=True)
            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)
            (rotary_emb): Qwen2RotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((3584,), eps=1e-06)
      (rotary_emb): Qwen2RotaryEmbedding()
      (h): ModuleList(
        (0-27): 28 x Qwen2DecoderLayer(
          (self_attn): Qwen2SdpaAttention(
            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)
            (k_proj): Linear(in_features=3584, out_features=512, bias=True)
            (v_proj): Linear(in_features=3584, out_features=512, bias=True)
            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)
            (rotary_emb): Qwen2RotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        )
      )
      (wte): Embedding(152064, 3584)
    )
  )
  (adpter): CNNSubsampling(
    (left_padding2): ConstantPad1d(padding=(4, 0), value=0.0)
    (conv1d2): Conv1d(1024, 2048, kernel_size=(5,), stride=(2,))
    (bn2): LayerNorm((2048,), eps=0.001, elementwise_affine=True)
    (relu2): GELU(approximate='none')
    (project): Linear(in_features=2048, out_features=3584, bias=True)
  )
  (task_embeddings): Embedding(20, 3584)
  (prompt_embeddings): Embedding(25, 3584)
  (predictor_head): Linear(in_features=3584, out_features=4, bias=True)
)