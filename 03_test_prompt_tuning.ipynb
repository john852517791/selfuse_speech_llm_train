{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch\n",
    "prompt_num = 25\n",
    "llm_embed_dim = 3584\n",
    "prompt_embeddings = nn.Embedding(prompt_num,llm_embed_dim)\n",
    "prompt_ids = torch.Tensor([i for i in range(prompt_num)]).long()\n",
    "prompt_ids.shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ids = torch.Tensor([i for i in range(prompt_num)]).long().unsqueeze(0).expand(2, -1)\n",
    "prompt_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 3584])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_embeddings(prompt_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_ids = {\n",
    "            \"sot\": 0,\n",
    "            \"transcribe\": 1,\n",
    "            \"translate\": 2,\n",
    "            \"zh\": 3,\n",
    "            \"en\": 4,\n",
    "            \"audio\": 5,\n",
    "            \"/audio\": 6,\n",
    "            \"hyps\": 7,\n",
    "            \"/hyps\": 8,\n",
    "        }\n",
    "task_ids[\"zh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.Tensor([0,1,2,3]).long()\n",
    "q1 = torch.Tensor([0,1,2]).long()\n",
    "q2 = torch.Tensor([3]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3584])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_q = prompt_embeddings(q)\n",
    "pp_q1 = prompt_embeddings(q1)\n",
    "pp_q2 = prompt_embeddings(q2)\n",
    "pp_q1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3584])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_q12 = torch.cat((pp_q1,pp_q2),dim=0)\n",
    "pp_q12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(pp_q12,pp_q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "freeze-omni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
